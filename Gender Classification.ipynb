{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classificaton\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from scrape import get_remote_tgz_files, download_extract_files\n",
    "from parse import paths, parse_readme\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "from dbhelpers import select\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from features import extract_mfcc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_URL = '''http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Audio/Main/16kHz_16bit/'''\n",
    "DATA_DIR = '''data'''\n",
    "DB = '''genderclass.db'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data\n",
    "Only files not already downloaded and extracted will be fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(DATA_DIR)\n",
    "urls = get_remote_tgz_files(SOURCE_URL)\n",
    "download_extract_files(urls, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DB to hold features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB)\n",
    "db = conn.cursor()\n",
    "db.execute('''DROP TABLE IF EXISTS features''')\n",
    "db.execute('''CREATE TABLE features\n",
    "                     (file text, \n",
    "                      path text, \n",
    "                      female integer, \n",
    "                      age text, \n",
    "                      language text, \n",
    "                      dialect text,\n",
    "                      cepstrum blob,\n",
    "                      n_mfcc integer,\n",
    "                      n_frames integer)''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "Silence trimmed at beginning and end of each file.\n",
    "\n",
    "MFCC extracted (with default librosa settings).\n",
    "\n",
    "Save into database.\n",
    "\n",
    "(skipped 127 records due to lack of parsable readme or wav-folder, takes about 20 min to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-25 21:43:11.750068\n",
      "2019-03-25 22:04:53.244463\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "data_folder = Path(DATA_DIR)\n",
    "conn = sqlite3.connect(DB)\n",
    "db = conn.cursor()\n",
    "samples = os.listdir(data_folder)\n",
    "\n",
    "skipped = []\n",
    "for sample in samples:\n",
    "    (wav_folder, readme_path) = paths(data_folder, sample)\n",
    "\n",
    "    if(wav_folder is None or readme_path is None):\n",
    "        skipped.append(wav_folder)\n",
    "        continue;\n",
    "\n",
    "    meta = parse_readme(readme_path)\n",
    "    if(not 'gender' in meta):\n",
    "        skipped.append(wav_folder)\n",
    "        continue;\n",
    "    try:\n",
    "        wavs = os.listdir(wav_folder)\n",
    "        for wav in wavs:\n",
    "            y, fs = librosa.load(wav_folder / wav, sr=None)\n",
    "            y, idx = librosa.effects.trim(y)\n",
    "            mfcc = librosa.feature.mfcc(y, fs)\n",
    "\n",
    "            db.execute('''INSERT INTO features VALUES\n",
    "             (?,?,?,?,?,?,?,?,?)''', \n",
    "                (wav[:-4], \n",
    "                str(wav_folder), \n",
    "                meta['gender'] == 'female', \n",
    "                meta.get('age range', ''), \n",
    "                meta.get('language', ''), \n",
    "                meta.get('pronunciation dialect',''),\n",
    "                bytes(memoryview(mfcc)),\n",
    "                mfcc.shape[0],\n",
    "                mfcc.shape[1]))\n",
    "\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(wav_folder)\n",
    "        print(\"type error: \" + str(e))\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "conn.close()\n",
    "print(datetime.datetime.now())\n",
    "print(len(skipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into data sets\n",
    "Known problem: Will not return same data set every time since sqlite does not support seeded random. Should be done in python instead.\n",
    "\n",
    "Trade off: unbalanced data set with most subjects having 10 files, but there are those with a lot more (max 530). Only 430 females and over 5000 males. Optimally would be good if not same subject was present in training and test set, and if subjects were sampled to an equal representation, but this will have to do for now! (target has been equal sampling of male/female). \n",
    "\n",
    "Could create validation data set by same (improved) method if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB)\n",
    "db = conn.cursor()\n",
    "females = select('''SELECT * FROM features WHERE female == 1 ORDER BY RANDOM()''', db)\n",
    "males = select('''SELECT * FROM features WHERE female == 0 ORDER BY RANDOM() LIMIT {}'''.format(len(females)), db)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "l = int(len(females) / 2)\n",
    "\n",
    "train = (males[0:l], females[0:l])\n",
    "test = (males[l+1:2*l], females[l+1:2*l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train one GMM model per gender\n",
    "Also, clips are not the same length so a fixed number of random frames are selected per clip.\n",
    "\n",
    "Skip first coefficient since that represents average power, but normalize before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bdb55b560607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#log_likelihood = np.zeros(len(models))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#gmm    = models[i]         #checking with each model one by one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arrange' is not defined"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "frames_per_speaker = 2\n",
    "for gender in train:\n",
    "    features = extract_mfcc_vectors(gender, frames_per_speaker)\n",
    "    \n",
    "    gmm = GMM(n_components = 4, max_iter = 200, covariance_type='diag',n_init = 3)\n",
    "    gmm.fit(features)\n",
    "    \n",
    "    models.append(gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3e1f1be8db84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgmm\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m         \u001b[0;31m#checking with each model one by one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#log_likelihood[i] = scores.sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#winner = np.argmax(log_likelihood)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "#for gender in np.arange(len(models)):\n",
    "\n",
    "scores = None\n",
    "log_likelihood = np.zeros(len(models))\n",
    "\n",
    "    features = extract_mfcc_vectors(test[i], frames_per_speaker)\n",
    "\n",
    "for i in range(len(models)):\n",
    "    \n",
    "    gmm    = models[i]         #checking with each model one by one\n",
    "    scores = np.array(gmm.score(features))\n",
    "    print(len(scores))\n",
    "    #log_likelihood[i] = scores.sum()\n",
    "#winner = np.argmax(log_likelihood)\n",
    "\n",
    "#print(winner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remember taht you might have to transpose mfcc to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Possibly rewisit narrower frames:\n",
    "#spec = librosa.feature.melspectrogram(y=y, sr=fs, S=None, n_fft=400, hop_length=160, power=2.0)\n",
    "#mfcc1 = librosa.feature.mfcc(y, fs, S=numpy.log(spec))\n",
    "\n",
    "#librosa.display.specshow(mfcc1, sr=fs, x_axis='time', hop_length=160)\n",
    "#librosa.display.specshow(mfcc2, sr=fs, x_axis='time')\n",
    "\n",
    "#IPython.display.Audio(y, rate=fs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#db.execute('''SELECT * FROM features''')\n",
    "#res = db.fetchall()\n",
    "#print(res)\n",
    "#mfcc2 = numpy.frombuffer(res[0][6])\n",
    "#mfcc2 = numpy.reshape(mfcc2,(res[0][7], res[0][8]))\n",
    "#librosa.display.specshow(mfcc2, sr=16000, x_axis='time')\n",
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Improvements\n",
    "\n",
    "ML:\n",
    "- Make use of other features - potentially other classification methods\n",
    "- Narrower frames of e.g. 25 ms\n",
    "#spec = librosa.feature.melspectrogram(y=y, sr=fs, S=None, n_fft=400, hop_length=160, power=2.0)\n",
    "#mfcc1 = librosa.feature.mfcc(y, fs, S=numpy.log(spec))\n",
    "\n",
    "Coding\n",
    "- Error and type checks in functions, can really only be used in known context\n",
    "- Latter workbook cells not independent, should use db between cells and not variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Thoughts:\n",
    "Fit one gmm per class instead of one gaussian per class and evalate likelihood since there the MFCCs per gender wont be one gaussian each? Could then just have used median frequency?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
